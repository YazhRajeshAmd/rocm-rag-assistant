# ROCm RAG Assistant

ğŸš€ **Multi-LLM ROCm Installation & Troubleshooting Assistant with RAG Architecture**

A sophisticated Retrieval-Augmented Generation (RAG) system specifically designed for AMD ROCm installation, configuration, and troubleshooting. Features multi-LLM architecture with parallel processing for optimal performance.

## ğŸ¯ Features

### ğŸ§  Multi-LLM Architecture
- **4 Parallel Ollama Instances**: Load-balanced across multiple GPU instances
- **Smart Response Selection**: Chooses the best response based on quality metrics
- **Performance Monitoring**: Real-time metrics and instance status tracking
- **Graceful Fallbacks**: Continues operation even if some instances fail

### ğŸ“š Knowledge Base
- **ROCm Documentation Scraping**: Automatically fetches latest AMD ROCm docs
- **Vector Database**: ChromaDB with HuggingFace embeddings
- **Focused Content**: Installation, troubleshooting, and configuration guides
- **Smart Retrieval**: MMR (Maximal Marginal Relevance) for diverse, relevant results

### ğŸ¨ Modern Interface
- **Beautiful Gradio UI**: Professional gradient design with responsive layout
- **Interactive Chat**: Real-time conversation with expert-level responses
- **Example Prompts**: Common ROCm questions for quick access
- **Performance Metrics**: Live display of LLM instance performance

## ğŸ› ï¸ Technology Stack

- **Python 3.8+**
- **LangChain**: RAG orchestration and document processing
- **Ollama**: Multi-instance LLM serving (Llama 3.1 8B)
- **ChromaDB**: Vector database for semantic search
- **HuggingFace**: Sentence transformers for embeddings
- **BeautifulSoup**: Web scraping for documentation
- **Gradio**: Modern web interface

## ğŸš€ Quick Start

### Prerequisites

1. **AMD GPU with ROCm Support**
2. **Ollama Installation**:
   ```bash
   curl -fsSL https://ollama.com/install.sh | sh
   ollama pull llama3.1:8b
   ```

### Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/yourusername/rocm-rag-assistant.git
   cd rocm-rag-assistant
   ```

2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

3. Start multiple Ollama instances:
   ```bash
   cd gpu1
   chmod +x start_ollama.sh
   ./start_ollama.sh
   ```

4. Run the enhanced assistant:
   ```bash
   python rocm_assistant_enhanced.py
   ```

5. Access the web interface at `http://localhost:7863`

## ğŸ“Š Architecture Overview

### Multi-Instance Setup
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Ollama :11438 â”‚    â”‚   Ollama :11439 â”‚    â”‚   Ollama :11440 â”‚    â”‚   Ollama :11441 â”‚
â”‚  Llama 3.1 8B   â”‚    â”‚  Llama 3.1 8B   â”‚    â”‚  Llama 3.1 8B   â”‚    â”‚  Llama 3.1 8B   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚                       â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚              RAG Assistant                       â”‚
                    â”‚          Smart Load Balancer                    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚              Vector Database                     â”‚
                    â”‚         (ChromaDB + Embeddings)                 â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### RAG Pipeline
1. **Document Ingestion**: Scrapes latest ROCm documentation
2. **Text Chunking**: Splits documents with optimal overlap
3. **Embedding Generation**: Creates semantic vectors using HuggingFace
4. **Vector Storage**: Stores in ChromaDB with metadata
5. **Query Processing**: Retrieves relevant context using MMR
6. **Multi-LLM Generation**: Parallel response generation
7. **Response Selection**: Chooses best response based on quality metrics

## ğŸ¯ Use Cases

### Installation Support
- **Fresh ROCm Installation**: Step-by-step guides for different Linux distributions
- **Package Management**: Repository setup, dependency resolution
- **Version Compatibility**: Matching ROCm versions with GPU architectures

### Configuration & Optimization
- **Environment Variables**: HIP_VISIBLE_DEVICES, ROCM_PATH setup
- **Performance Tuning**: Memory allocation, compute optimization
- **Multi-GPU Setup**: Configuring multiple AMD GPUs

### Troubleshooting
- **Installation Errors**: Package conflicts, dependency issues
- **Runtime Problems**: GPU detection, memory errors
- **Performance Issues**: Optimization recommendations

## ğŸ“ Project Structure

```
rag_model/
â”œâ”€â”€ gpu1/
â”‚   â”œâ”€â”€ rocm_assistant_enhanced.py    # Enhanced multi-LLM assistant
â”‚   â”œâ”€â”€ gpu1.py                       # Original multi-panel version
â”‚   â”œâ”€â”€ start_ollama.sh              # Multi-instance startup script
â”‚   â”œâ”€â”€ rocm_kb/                     # Vector database storage
â”‚   â””â”€â”€ logs/                        # Ollama instance logs
â”œâ”€â”€ gpu4/
â”‚   â”œâ”€â”€ gpu4.py                      # GPU4-specific version
â”‚   â”œâ”€â”€ gpu4_metrics.py              # Performance monitoring
â”‚   â””â”€â”€ start_ollama_gpu4.sh         # GPU4 startup script
â”œâ”€â”€ requirements.txt                  # Python dependencies
â””â”€â”€ README.md                        # This file
```

## ğŸ”§ Configuration

### Multi-Instance Setup
```bash
# Edit start_ollama.sh to configure:
GPU_TO_USE=5                    # AMD GPU index
ports=(11438 11439 11440 11441) # Ollama ports
```

### RAG Parameters
```python
# Vector database configuration
chunk_size=1000                 # Document chunk size
chunk_overlap=200              # Overlap between chunks
search_k=5                     # Retrieved documents per query
```

### LLM Settings
```python
# Ollama configuration
model="llama3.1:8b"            # Model name
temperature=0.1                # Response randomness (low for technical accuracy)
```

## ğŸš€ Advanced Features

### Performance Monitoring
- **Response Time Tracking**: Per-instance timing metrics
- **Quality Assessment**: Response length and completeness scoring
- **Load Balancing**: Automatic distribution across healthy instances
- **Health Checks**: Continuous monitoring of instance availability

### Smart Response Selection
- **Quality Metrics**: Length, completeness, and relevance scoring
- **Performance Weighting**: Faster responses get priority in ties
- **Fallback Handling**: Graceful degradation when instances fail
- **Source Attribution**: Tracks which documentation sources were used

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/rocm-enhancement`)
3. Commit your changes (`git commit -m 'Add ROCm enhancement'`)
4. Push to the branch (`git push origin feature/rocm-enhancement`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ”— Related Projects

- [Financial Stock Analysis](https://github.com/yourusername/fsi-stock-analysis)
- [MRI Analysis Tool](https://github.com/yourusername/mri-analysis-tool)

## ğŸ“š Documentation Sources

- [AMD ROCm Documentation](https://rocm.docs.amd.com/)
- [ROCm Installation Guide](https://rocm.docs.amd.com/en/latest/deploy/linux/quick_start.html)
- [System Requirements](https://rocm.docs.amd.com/en/latest/deploy/linux/prerequisites.html)

## âš¡ Performance

- **Multi-LLM Processing**: 4x parallel processing capability
- **Response Time**: < 5 seconds for most queries
- **Throughput**: Handles multiple concurrent users
- **Accuracy**: Expert-level ROCm knowledge with source attribution

---

**Built with â¤ï¸ for the AMD ROCm community**
